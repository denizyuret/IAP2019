{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Recurrent Neural Networks\n",
    "(c) Deniz Yuret, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Objectives: learn about RNNs, the RNN layer, compare with MLP on a tagging task.\n",
    "* Prerequisites: param, relu, train!, nll, zeroone, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "for p in (\"Knet\",\"ProgressMeter\",\"Plots\")\n",
    "    haskey(Pkg.installed(),p) || Pkg.add(p)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Brown Corpus\n",
    "To introduce recurrent neural networks (RNNs) we will train a part-of-speech tagger using the [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus). We will train three models: a MLP, a unidirectional RNN, a bidirectional RNN and observe significant performance differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet: Knet\n",
    "include(Knet.dir(\"data/nltk.jl\"))\n",
    "(data,words,tags) = brown()\n",
    "println(\"The Brown Corpus has $(length(data)) sentences, $(sum(length(p[1]) for p in data)) tokens, with a word vocabulary of $(length(words)) and a tag vocabulary of $(length(tags)).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data` is an array of `(w,t)` pairs each representing a sentence, where `w` is a sequence of word ids, and `t` is a sequence of tag ids. `words` and `tags` contain the strings for the ids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.((data,words,tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the first sentence looks like with ids and with strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV[\"COLUMNS\"]=120\n",
    "(w,t) = first(data)\n",
    "display(permutedims([w t]))\n",
    "display(permutedims([words[w] tags[t]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "`data` has each sentence tokenized into an array of words and each word mapped to a `UInt16` id. To use these words as inputs to a neural network we further map each word to a Float32 vector. We will keep these vectors in the columns of a size (X,V) matrix where X is the embedding dimension and V is the vocabulary size. The vectors will be initialized randomly, and trained just like any other network parameter. Let's define an embedding layer for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet: param\n",
    "struct Embed; w; end\n",
    "Embed(vocabsize::Int,embedsize::Int) = Embed(param(embedsize,vocabsize))\n",
    "(e::Embed)(x) = e.w[:,x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the words, word ids and embeddings for a sentence looks like: (note the identical id and embedding for the 2nd and 5th words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedlayer = Embed(length(words),8)\n",
    "(w,t) = data[52855]\n",
    "display(permutedims(words[w]))\n",
    "display(permutedims(w))\n",
    "display(embedlayer(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected (Linear) layer with optional activation function at the end\n",
    "struct FC; w; b; f; end\n",
    "FC(input::Int,output::Int,f=identity) = FC(param(output,input),param(output),f)\n",
    "reshape2d(x) = reshape(x,(size(x,1),:))\n",
    "(fc::FC)(x;o...) = fc.f.(fc.w * reshape2d(x) .+ fc.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define these manually, for now just check out @doc RNN\n",
    "using Knet: RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The three taggers: MLP, RNN, biRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet: relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A chain of layers\n",
    "struct Chain; layers; end\n",
    "Chain(layer1,layer2,layers...)=Chain((layer1,layer2,layers...))\n",
    "(c::Chain)(x;o...) = (for l in c.layers; x = l(x;o...); end; x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tagger0(vocab,embed,hidden,output)=  # MLP Tagger\n",
    "    Chain(Embed(vocab,embed),FC(embed,hidden,relu),FC(hidden,output))\n",
    "Tagger1(vocab,embed,hidden,output)=  # RNN Tagger\n",
    "    Chain(Embed(vocab,embed),RNN(embed,hidden,rnnType=:relu),FC(hidden,output))\n",
    "Tagger2(vocab,embed,hidden,output)=  # biRNN Tagger\n",
    "    Chain(Embed(vocab,embed),RNN(embed,hidden,rnnType=:relu,bidirectional=true),FC(2hidden,output));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagger0 (MLP)\n",
    "This is what Tagger0 looks like. Every tag is predicted independently. The prediction of each tag only depends on the corresponding word.\n",
    "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vTfV4-TB0KwjDbFKpj3rL0tfeApEh9XXaDJ1OF3emNVAmc_-hvgqpEBuA_K0FsNuxymZrv3ztScXxqF/pub?w=378&h=336\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagger1 (RNN) \n",
    "This is what Tagger1 looks like. The RNN layer takes its previous output as an additional input. The prediction of each tag is based on words to the left.\n",
    "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vTaizzCISuSxihPCjndr7xMVwklsrefi9zn7ZArCvsR8fb5V4DGKtusyIzn3Ujp3QbAJgUz1WSlLvIJ/pub?w=548&h=339\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagger2 (biRNN)\n",
    "This is what Tagger2 looks like. There are two RNNs: the forward RNN reads the sequence from left to right, the backward RNN reads it from right to left. The prediction of each tag is dependent on all the words in the sentence.\n",
    "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vQawvnCj6odRF2oakF_TgXd8gLxSsfQP8-2ZdBdEIpfgIyPq0Zp_EF6zcFJf6JlGhfiKQvdVyg-Weq2/pub?w=566&h=335\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Minibatching\n",
    "Minibatching is a bit more complicated with sequences compared to simple classification problems, this section can be skipped on a first reading. In addition to the input and minibatch sizes, there is also the time dimension to consider. To keep things simple we will concatenate all sentences into one big sequence, then split this sequence into equal sized chunks. The input to the tagger will be size (B,T) where B is the minibatch size, and T is the chunk size. The input to the RNN layer will be size (X,B,T) where X is the embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 64\n",
    "SEQLENGTH = 32;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function seqbatch(x,y,B,T)\n",
    "    N = length(x) รท B\n",
    "    x = permutedims(reshape(x[1:N*B],N,B))\n",
    "    y = permutedims(reshape(y[1:N*B],N,B))\n",
    "    d = []; for i in 0:T:N-T\n",
    "        push!(d, (x[:,i+1:i+T], y[:,i+1:i+T]))\n",
    "    end\n",
    "    return d\n",
    "end\n",
    "allw = vcat((x->x[1]).(data)...)\n",
    "allt = vcat((x->x[2]).(data)...)\n",
    "d = seqbatch(allw, allt, BATCHSIZE, SEQLENGTH);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be a bit more clear if we look at an example minibatch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,y) = first(d)\n",
    "words[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia indexing allows us to get the embeddings for this minibatch in one go as an (X,B,T) array where X is the embedding size, B is the minibatch size, and T is the subsequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedlayer = Embed(length(words),128)\n",
    "summary(embedlayer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle and split minibatches into train and test portions\n",
    "using Random; shuffle!(d)\n",
    "dtst = d[1:10]\n",
    "dtrn = d[11:end]\n",
    "length.((dtrn,dtst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For running experiments\n",
    "using Knet: train!, nll, zeroone, Adam, AutoGrad\n",
    "import ProgressMeter\n",
    "\n",
    "function trainresults(file,model; o...)\n",
    "    if (print(\"Train from scratch? \");readline()[1]=='y')\n",
    "        results = Float64[]; updates = 0; prog = ProgressMeter.Progress(2500)\n",
    "        function callback(J)\n",
    "            if updates % 100 == 0\n",
    "                push!(results, nll(model,dtst), zeroone(model,dtst))\n",
    "                ProgressMeter.update!(prog, updates)\n",
    "            end\n",
    "            return (updates += 1) <= 2500\n",
    "        end\n",
    "        train!(model, dtrn; callback=callback, optimizer=Adam(), o...)\n",
    "        results = reshape(results,(2,:))\n",
    "        Knet.gc()\n",
    "        Knet.save(file,\"model\",model,\"results\",results)\n",
    "    else\n",
    "        isfile(file) || download(\"http://people.csail.mit.edu/deniz/models/tutorial/$file\",file)\n",
    "        model,results = Knet.load(file,\"model\",\"results\")\n",
    "    end\n",
    "    println(minimum(results,dims=2))\n",
    "    return (model,results)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABSIZE = length(words)\n",
    "EMBEDSIZE = 128\n",
    "HIDDENSIZE = 128\n",
    "OUTPUTSIZE = length(tags);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(t0,r0) = trainresults(\"tagger0.jld2\",Tagger0(VOCABSIZE,EMBEDSIZE,HIDDENSIZE,OUTPUTSIZE));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(t1,r1) = trainresults(\"tagger1.jld2\",Tagger1(VOCABSIZE,EMBEDSIZE,HIDDENSIZE,OUTPUTSIZE));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(t2,r2) = trainresults(\"tagger2.jld2\",Tagger2(VOCABSIZE,EMBEDSIZE,HIDDENSIZE,OUTPUTSIZE));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots; default(fmt=:png,ls=:auto,ymirror=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([r0[2,:], r1[2,:], r2[2,:]]; xlabel=\"x100 updates\", ylabel=\"error\",\n",
    "    ylim=(0,0.15), yticks=0:0.01:0.15, labels=[\"MLP\",\"RNN\",\"biRNN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([r0[1,:], r1[1,:], r2[1,:]]; xlabel=\"x100 updates\", ylabel=\"loss\",\n",
    "    ylim=(0,.5), yticks=0:0.1:.5, labels=[\"MLP\",\"RNN\",\"biRNN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n",
    "Below, you can type and tag your own sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdict=Dict{String,UInt16}(); for (i,w) in enumerate(words); wdict[w]=i; end\n",
    "unk = UInt16(length(words))\n",
    "wid(w) = get(wdict,w,unk)\n",
    "function tag(tagger,s::String)\n",
    "    w = permutedims(split(s))\n",
    "    t = tags[(x->x[1]).(argmax(Array(tagger(wid.(w))),dims=1))]\n",
    "    vcat(w,t)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag(t2,readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "julia.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
